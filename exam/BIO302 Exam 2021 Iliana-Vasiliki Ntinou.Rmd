---
title: "BIO302 Exam 2021"
author: "Iliana Vasiliki Ntinou"
date: "21st of June 2021"
output: html_document
bibliography: bibliography.bibtex
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Answers

### 1) Discuss the advantages and challenges of pre-registering an experiment.

Pre-registering an experiment refers to the concept of a data analysis of a research to be published prior to obtaining the results. This is beneficial for many reasons, especially since nowadays, we have seen examples of p-hacking, data mining, and in general manipulations that make research unreproducible (Reproducibility crisis Baker). To start, preregistration can limit publication bias towards more positive findings. It can prevent fishing and manipulations as mentioned above, and therefore the researcher can choose a model/analysis that is well reasoned based on theory before collecting the data. Furthermore, it can help balance the publication bias against null results. Lastly, pre-registering an  experiment can prevent third parties from pressuring for a certain result. On the other hand, preregistration comes with some challenges. For example, it is possible that something goes wrong in the procedure during the study, and pre-registration does not allow for diverting from what is already published. It is based on the concept of hypothesis testing, thus making pre reregistering an exploratory analysis challenging because the goal of exploration is to generate hypothesis and not test them. It is also possible that the data do not behave as you expected them to, or maybe the data you are using. Overall, preregistration contributes to increasing credibility and reproducibility (Nosek).

### 2) Discuss the steps you plan to use to make your thesis reproducible.

The concept of reproducibility has been discussed more and more often the last decades, due to the fact that modern computational tools are evolving so rapidly(goodman). Reproducibility can be used as a weapon against research malpractice(baker). As a master student, I will be soon doing my own research so in order to make sure that it is reproducible I can plan ahead. Firstly, I need to make a robust study design, with a clear aim and methods. Having good quality of data is the key for a good analysis. Secondly, I can make sure that any manipulation of that raw data (cleaning, formatting), all statistical analysis and plotting is done with code in an integrated development environment like Rstudio, under a project where I save all my files, have control of the working directory, and easy access to history and environment. Lastly, I can use [guides](https://www.britishecologicalsociety.org/publications/guides-to/) to make my code consistent and reproducible and use a version control like GitHub so I can share my work with others.

### 3) A statistical test has a p-value of 0.04. How should this p-value be interpreted? Is it good evidence against the null hypothesis?

The use of p-value in statistical testing is very common for determining the significance of a hypothesis. However, it happens to be that it can be misinterpreted or misused in many cases(Wasser, Amhrein).Traditionally, p-values are a much beloved tool for rejecting the null hypothesis. That, can hide several issues, like p-hacking and publication biases (Davey smith) which has lead to a reproducibility crisis (Baker). Thus, it is crucial to understand what it represents. A p-value is the probability of getting an affect at least as extreme as the one observed, assuming that the null hypothesis is true. Therefore, a p-value of 0.04 suggests that if there was no effect, we would get the observed difference or more in 4% of the times we would perform this test. The threshold of significance at 0.05 is set by no else but ourselves, so it could or could not be good evidence against the null hypothesis depending on where we set the threshold and what we find acceptable. Therefore if we want to be more precise at least we should acknowledge what a p-value does not tell us. It says nothing about the strength of an effect or its practical significance and it does not tell us that the alternative hypothesis is true. Additionally, the probability of finding "significance" increases by increasing the sample size, even if in reality the effect might be small and unimportant(Wassermein).

### 4) A graduate student got this advice from their supervisor: "Just make a giant correlation matrix and see what is interesting". Discuss the potential problems with this approach and how they could be resolved.[@RN235]

In this situation the student is doing an exploratory analysis. Their supervisor is advising him to try and find any potential important relationship considering all possible associations. This approach however, as much as attractive it may sound can be equally deceiving. There are very serious dangers that lurk behind multiple testing and lead to data dredging(davey smith):
  - There is high proneness for false discoveries (type I error). Since we have a giant correlation matrix, the number of tests that are going to be done in order to find significant results is also going to be very big. Therefore, we might end up with significant results that don't correspond to real significant result but were found by chance. For example, 5% of the correlations in a data set with randomly generated "covariates" will be significant by chance(Tenderick colquin).
  - Claiming significance when we are fishing for significance i.e. searching for interesting patterns then testing if they are statistically significant.
The best way to avoid these is to have a good study design in the first place or clearly state that the aim is generating and not testing hypothesis(Tendreneck). Other ways to correct for multiple testing is to p.adjust or use control criteria like FWER (probability that one or more significant covariates is actually spurious) or better FDR (expected proportion of significant results that are spurious).

### 5) Explain what autocorrelation is, how it can be detected and how its effect on regression can be controlled for.

Autocorrelation is a phenomenon where an observation of a variable is correlated to another observation in that same variable. We usually refer to temporal autocorrelation when we have repeated measurements i.e., when the value of the variable at t+1 is dependent on the value of the variable at t. There is also spatial autocorrelation, which describes the proneness of two areas that are close to one another that we see as similar observations. In general, autocorrelation is present when observations are dependent on an aspect. This of course is a problem for regression analyses because it violates one of its assumptions, that the residuals are independent. Autocorrelation affects the standard errors and but leaves the coefficients unbiased. If not accounted for, it can also lead to type I error of false rejecting the null hypothesis. One way to test for autocorrelation is to do a Durbin-Watson test. The Durbin-Watson statistic ranges from 0 to 4. A result close to 0 or 4 implies an important positive or negative autocorrelation respectively, and values close to 2 mean less autocorrelation. There are many ways to control the effect of autocorrelation on the regression analysis. Typically, we use a 1st order autoregressive process corAR1(). There are other correlation structures that can be incorporated in a model like corARMA which stands for autoregressive moving average process and corCAR1 that represents a continuous autoregressive process. The [choice of an autocorrelation structure](http://finzi.psych.upenn.edu/R/library/stats/html/ar.html)) can be challenging , however, choosing the wrong structure is still better than assuming no autocorrelation at all.

### 6) Model the relationship between total plant biomass in alpine grasslands and summer air temperature in China. Available data are biomass per species per plot. There are \~15 plots in each of four sites. Each site is at a different elevation and has a climate logger.

-   Climate data can be downloaded from <https://osf.io/34qnr/>. Biomass data can be downloaded from <https://osf.io/6sfqw/>.

```{r, warning=FALSE, message=FALSE}
#Download and import the climate data which are in csv format. 
library(knitr)
library(tidyverse)
library(here)

url = "https://osf.io/34qnr/download"
download.file (url, "China_2013_2016_AirTemp_month.csv")
climate.df <- read_csv(here("exam/China_2013_2016_AirTemp_month.csv"))


#Download and import the biomass data which are in excel format. The biomass data can be attained from this https://osf.io/6sfqw/ url but it's best to download it manually and save it in the working folder so that all sheets are saved in the same file. 
library(readxl)

biomass.df <- excel_sheets(path = "biomass2015.xls") %>% 
                map_dfr(~read_excel(path = "biomass2015.xls", sheet =.x))
```

-   Calculate mean summer air temperatures each site. Use the logger "gradient". The OTC logger is part of another experiment.

```{r, message=FALSE}
library(lubridate)

climate <- climate.df %>% 
                    #convert the date so it is easier to choose the summer months
                    mutate(date= ymd(month)) %>% 
                    #set site as factor
                    mutate(site = as.factor(site)) %>% 
                    #we need to filter the data to include only summer months and gradient logger  
                    filter(month(date)>05, month(date)<09, logger=="gradient") %>% 
                    #group the values by site  
                    group_by(site) %>%  
                    #calculate the mean temperature
                    summarise(meansummertemp=mean(value, na.rm=T))

```

If we were interested to look at how the mean summer air temperature of 2015 affected that years biomass then we would select the data only from 2015. That could be done as presented below:

```{r, message=FALSE}
climate2015 <-climate.df %>% 
              mutate(date= ymd(month)) %>% 
              mutate(site = as.factor(site)) %>% 
              #we need to filter the data to include only summer months of 2015
              filter(month(date)>05, month(date)<09, year(date)==2015, logger=="gradient") %>%  
              group_by(site) %>% 
              summarise(meansummertemp=mean(value, na.rm=T))
```

In this case we choose to continue with the overall mean summer air temperature from data taken from all the years in the dataset.Here the question is to model the relationship between biomass and mean summer air temperature. Having data from many years would give us a better picture of what the mean summer air temperature is in a place, because it could for example happen that one year there was an unusual phenomenon that led to the mean temperature of that year being an extreme.In the latter case, we would be examining whether that phenomenon that affected that years temperature is related to that year's production.

-   Calculate biomass per plot.

```{r, message=FALSE}
biomass <- biomass.df %>%
              #choose columns we are interested in
              dplyr::select(c(site,plot, production)) %>% 
              #set site as factor
              mutate(site = as.factor(site), 
              #set plot as factor
              plot= as.factor(plot)) %>% 
              #group the variable
              group_by(site, plot) %>% 
              #calculate total biomass per plot
              summarise(biomass = sum(production, na.rm = T))
```

-   Join the climate data to the biomass data.

```{r, message=FALSE}
#take all rows from biomass and matching rows from climate
bc <- left_join(biomass,climate, by="site")%>%
      dplyr::select(c(site,biomass, meansummertemp))
```

-   Choose and fit a suitable model to find the relation a biomass and mean summer temperature.

Firstly, we can check for any strong association between the variables in our dataset.

```{r, warning=FALSE, message=FALSE}
library(GGally)

GGally::ggpairs(bc, cardinality_threshold = 20)
```

Mean summer temperature and site have the strongest correlation. That is expected since we have one value for the temperature per site. We also see that the correlation between biomass and mean temperature is about the same with the correlation of biomass with site. But because we are not fishing for significance we must remember that when working with correlations we shall never assume that a change in one variable causes a change in another ([spurious correlations](https://www.tylervigen.com/spurious-correlations)). It means that there is a positive correlation but we don't know if the change in temperature causes the change in biomass. Anyway these results above may help us get a better view of our data.

To choose a regression model we assume the data follow a normal distribution since the response variable (biomass) is continuous. The predictor variable (mean summer temperature) is also continuous, and the data are clustered in four sites. In that case we choose linear mixed effect model with random effect the sites. Since the research question is to find best model that fits the relationship between biomass and temperature, we assume that site is a random factor. In general, if an effect is fixed or random is mainly the decided by the research question or is decided by the researcher (Schabenberger and Pierce 2001).

```{r, warning=FALSE, message=FALSE}
library(nlme)

#First we build a list of candidate models
mods <- list()
    #null model: biomass isn't related to any variable
    mods$fit.lme <- lme(biomass~+1, random=~+1|site, data=bc)
    #linear mixed effect model
    mods$fit0.lme <- lme(biomass~meansummertemp, random=~+1|site, data=bc)
    #linear mixed effect model with weighted variance. We have to account for heteroskedasticity
    #(the   variance for all observations in each site is not the same)
    mods$fit1.lme <- lme(biomass~meansummertemp, random=~+1|site,
                         weights = varIdent(form=~+1|site),data=bc)

#Then we extract the AIC from each model. The best model is the one with the smallest AIC
library(AICcmodavg)
    
sapply(mods, AIC)

#Finally, we calculate the deltaAIC for each model and the AIC weights for each model.
#ΔAIC - difference between the best AIC and AIC of the other models. 
#Best model has a ΔAIC of zero, highest values for worst models.
aictab(mods)

#Alternatively we can compare the three models with anova.

fit.lme <- lme(biomass~+1, random=~+1|site, data=bc)
  
fit0.lme <- lme(biomass~meansummertemp, random=~+1|site, data=bc)

fit1.lme <- lme(biomass~meansummertemp, random=~+1|site, weights = varIdent(form=~+1|site), data=bc)

anova(fit.lme, fit0.lme, fit1.lme)
#The improvement is significant.

```

The model fit1.lme is the best option as it holds 99% of the cumulative model weight and has the lowest AIC(642.5004). The ΔAIC for the other two models are very high (\<10), which does not make them good options.

-   Check the model's assumptions are met.

```{r  fig.height=8, fig.width=8, message=FALSE, warning=FALSE}
#Are the residuals well behaved?
#Is the response variable a reasonably linear function of the fitted values?
#Are the errors reasonably close to normally distributed?
library(qqplotr)
library(see)
library(JWileymisc)

performance::check_model(fit1.lme)
```

The model diagnostics look ok!

-   Report key statistics from your model in a table or in the text.

```{r, warning=FALSE, message=FALSE}
#look at the anova and summary output
anova(fit1.lme, type="m")
summary(fit1.lme)

#get a small report of the model's statistics
library(report)

report::report(fit1.lme)
```

Note that we only have 2 degrees of freedom for mean summer temperature. This is because we only have 4 independent observations of i.e. 4 sites. The F-value or the ratio of explained variance to unexplained variance by the model is 1.9, which is not very large. The p value (0.301) is the probability of obtaining an F value as extreme or more extreme as the one observed given that the null hypothesis is true. In our case the p value is in favor of the null hypothesis. Variance between sites is bigger than the residual variance or variance within sites) (19.34027 \> 18.45851). The beta values are the estimates of the fixed effects, so in this case it is 7.11. The std beta or [standardized beta coefficient](Stephanie Glen. "Standardized Beta Coefficient: Definition & Example" From StatisticsHowTo.com: Elementary Statistics for the rest of us! <https://www.statisticshowto.com/standardized-beta-coefficient/>) which is 0.48 in our case, means that with every increase of one standard deviation in mean summer temperature, biomass increases by 0.48 standard deviations.

-   Make a publication quality plot that shows the relationship between biomass and mean summer temperature.

```{r}
library(tidyverse)

p1 <- ggplot(bc, aes( x=meansummertemp, y=biomass, colour=site, fill=site))+
      
      geom_boxplot()+
      
      stat_summary(fun=mean, geom="point", shape=21, size=6)+
      
      ylim(0, 150)+
  
      scale_colour_manual(values= c("#7570B3","#E7298A","#E6AB02", "#A6761D"), 
                          breaks = c("H", "A", "M", "L"), labels = c("High alpine", "Alpine", "Middle", "Lowland")) +
      
      scale_fill_manual(values= alpha(c("#7570B3","#E7298A","#E6AB02", "#A6761D"), .3), 
                        breaks = c("H", "A", "M", "L"), labels = c("High alpine", "Alpine", "Middle", "Lowland"))+
      
      ggtitle("Relationship between biomass and mean summer temperature in alpine grasslands")+
      
      labs(x="Mean Summer Air Temperature (C°)", y="Biomass", col="Site", fill="Site")+
      
      theme_classic(base_size=14)+
      
      theme(plot.title = element_text(hjust=0.299, size=14),
            
      axis.text.x = element_text(colour="black"), 
            
      axis.text.y = element_text(colour="black"), 
            
      legend.text = element_text(colour="black"),   
            
      panel.grid.minor.x= element_blank(), panel.grid.major.x=element_blank(),
            
      panel.grid.minor.y= element_blank(), panel.grid.major.y=element_blank(),
            
      strip.background=element_blank())
p1
```

*Figure 1. Total plant biomass in alpine grasslands and mean summer air temperature in China, measured from four sites. The error bars on the boxplot represent the range from minimum to maximum values of total biomass measured per plot. The circle represents the mean of each group, and the horizontal line the median.*

-   Write the statistical part of the methods and results sections of a manuscript describing the biomass-climate relationship. You should justify your choice of model.

**Data analysis**

Total plant biomass per plot per site from 2015 and mean summer air temperature per site were calculated. A data matrix was produced to look for any strong association patterns between the different variables and help visualize the data. A linear mixed effect regression was chosen with response variable the biomass, fixed effect the mean summer air temperature and random effect the site. Three models were compared: a null model with no fixed effetcs, a linear mixed effect model and a weihted linear mixed effect model because of presence of heteroscedasticity in the data. Model selection was based on the Akaike’s Information Criterion(AIC). Out of the three options, the model with the lowest AIC score was selected, and the model's assumptions were checked by looking at the diagnostics plots.The data were presented in a boxplot. All analyses and plots were done in Rmarkdown using Rversion 4.1.0. The methodology and code for the analysis is available at GitHub()


-   Write a biological interpretation of your final model.

    -   According to the final model there is no significant effect of mean summer air temperature on the plant biomass. Despite the fact that we took several steps to choose the best fit model, it still does not practically describe the real situation for many different reasons. In other words, a non-sig-nificant effect can refer to a list of things like a true null effect, a real underpowered effect,  or  an  obscure  effect (Makin).
    -   First of all, the way the dataset is built is causing many problems, since we only have one value for the temperature per site, which means we can't see in any way whether and how the biomass is affected by temperature within each site, because we don't have any observations for temperature per plot as we have for biomass. Alternatively, if we didn't calculate the biomass per plot but only by site and then looked at the relationship between biomass and mean summer air temperature per site, we would have only 4 observations, which is not a good sample size to perform any statistical analysis.
    -   From the above we can conclude , that even if there is an effect of mean summer air temperature on plant biomass in reality, there is no way we can find it let alone model it correctly with this type of data. What this data shows is that total biomass is different between the four sites but we have no clue whether it is because of the temperature or any other factor, which brings me to my next point.
    -   According to the research question there are no other data about any other climatic factor that may have may have an important impact on the plant biomass. So, when interpreting a result like this would be like ignoring all other factors. There are several climatic components like humidity, altitude etc., that together with air temperature affect plant biomass.
    -   Lastly, we can see that describing data in an aspect of significant and non-significant effects can completely misrepresent reality. In order to avoid such confusions, it is essential to have a strong experimental design, have a complete understanding of the research question and be aware of the power and weaknesses of different statistical analysis. Finally, this is a perfect example of reporting the actual findings based on the data and choosing to acknowledge the weaknesses of a method rather than trying to obtain a statistically significant result.(Wasserstein).

# References
